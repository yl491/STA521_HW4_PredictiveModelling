---
title: "HW4: Team [12]"
author: "Yunxuan Li, Wenxin Liao, Yan Zhao"
date: "Due October 13, 2017"
output:
  pdf_document: default
  html_notebook: default
---


```{r setup, echo=FALSE}
suppressMessages(library(ISLR))
suppressMessages(library(arm))
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages
```

This problem set has several dependent parts, so plan accordingly.  Here is a suggested outline to finish the assignment on time:

* Start problems 1-2, 4, 7 and 10 prior to Monday individually and with your group adding working code and minimal documentation for now.  It is important to get a head start on the model building.

* Try problem 3 and 6 prior to lab Wednesday so that you will be prepared to ask questions about simulation and coding with the goal of having a minimal working version for 3 and 6 by the end of lab.  This will help with the later questions where you apply it to the other models.  (work as much on those as well)

* don't forget midterm and take time to enjoy fall break

* Try problem 12, 14 and 15 before Lab on the 11th; use lab time to refine code, ask questions about interpretation, theory etc.  

* finish write up and turn in on Sakai on the 13th.  Please let us know if there are problems with missing packages for wercker as you go so that you have a passing badge.  Remove any instructions like this and above to clean up the presentation.

## Preliminaries

Load the college application data from Lab1 and create the variable `Elite` by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %.  We will also save the College names as a new variable and remove `Accept` and `Enroll` as temporally they occur after applying, and do not make sense as predictors in future data.

```{r data}
library(plyr)
library(dplyr)
data(College)
College = College %>% 
  mutate(college = rownames(College)) %>%
  mutate(Elite = factor(Top10perc > 50)) 
 # mutate(Elite = 
  #         recode(Elite, "TRUE" = "Yes", "FALSE"="No")) %>%
  #select(c(-Accept, -Enroll))
College$Elite<-mapvalues(College$Elite,from=c('TRUE','FALSE'),to=c("Yes","No"))
College=College%>%
  select(c(-Accept, -Enroll))

```

We are going to create a training and test set by randomly splitting the data.  First set a random seed by

```{r setseed}
# do not change this; for a break google `8675309`
set.seed(8675309)
n = nrow(College)
n.train = floor(.75*n)
train = sample(1:n, size=n.train, replace=FALSE)
College.train = College[train,]
College.test = College[-train,]
```



1. Create scatter plots of predictors versus `Apps` using the training data only.  If you use pairs or preferably `ggpairs` make sure that `Apps` is on the y-axis in plots versus the other predictors.  (Make sure that the plots are legible, which may require multiple plots.)  
Comment on any features in the plots, such as potential outliers, non-linearity, needs for transformations etc.
```{r}
ggpairs(College.train,columns=c(1,3:5,2))
ggpairs(College.train,columns=c(6:9,2))
ggpairs(College.train,columns=c(10:13,2))
ggpairs(College.train,columns=c(14:16,18,2))
#yl
```
\
Answer: outliers in undegrad, books, expends; potential predictors: f-undergrad, top 25 perc, phd(non-linear), terminal(non-linear), gradrate. those predictors with non-linear relationships many need to be transformed.
\

2.  Build a linear regression model to predict `Apps` from the other predictors using the training data.  Present model summaries and diagnostic plots.   Based on diagnostic plots  using residuals,  comment on the  adequacy of your model.
```{r}
College.train.rmca<-College.train
College.train.rmca$college<-NULL

College.test.rmca<-College.train
College.test.rmca$college<-NULL

Col.lm<-lm(Apps~.,data=College.train.rmca)
summary(Col.lm)
plot(Col.lm)

anova(Col.lm)
#YL
```
\
Answer: plot 1: non-random. needs to transform response and some of the predictors. plot 2: heavier tail - unexplained larger variance. plot 4: no influential points.\

#NEED ANALYSIS
3. Generate 1000 replicate data sets using the coefficients from the model you fit above.  Using RMSE as a statistic, $$\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.  Hint:  write a function to calculate RMSE.

#Need Analysis
```{r}

rmse=function(y,ypred)
{
  rmse=sqrt(mean((y-ypred)^2))
  return(rmse)
}
rm=c()
nsim=1000
n=nrow(College.train.rmca)
X=model.matrix(Col.lm)
sim.fit1=sim(Col.lm,nsim)
for (i in 1:nsim)
{
  mu=X %*% sim.fit1@coef[i,]
  y.rep=rnorm(n,mean=mu,sd=sim.fit1@sigma[i])
  newlm<-lm(y.rep~X)
  yhat<-predict.lm(newlm)
  rm[i]<-rmse(y.rep,yhat)
}



Col.lm.predict<-predict(Col.lm,data=College.train)

rmse.observations<-rmse(College.train$Apps,Col.lm.predict)
pval.lm1=sum(rm>rmse.observations)/length(rm)
pval.lm1
df.fit1<-data.frame(rmse.fit1=rm)
ggplot(df.fit1,aes(x=rmse.fit1))+geom_histogram()+geom_vline(xintercept=rmse.observations,col=2)



p_col1<-mean(rmse.observations<rm)

```
4. Build a second model, considering transformations of the response and predictors, possible interactions, etc with the goal of trying to achieve  a model where assumptions for linear regression are satisfied, providing justification for your choices.
Comment on  how well the assumptions are met and and issues that diagnostic plots may reveal.
```{r}

College.train.rmca$perc.alumni<-College.train.rmca$perc.alumni+1
College.train.rmca$Elite<-NULL
College.train.rmca$Private<-NULL
#bcn_k<-powerTransform(as.matrix(College.train.rmca[,-2])~.,family="bcnPower",data=College.train.rmca)
#boxTidwell(Apps~.,data=College.train.rmca)
library(car)
powerTransform(College.train.rmca)


College.train$Elite<-mapvalues(College.train$Elite,from=c("Yes","No"),to=c(2,1))
College.train$Private<-mapvalues(College.train$Private,from=c("Yes","No"),to=c(2,1))
Col.lm2<-lm(log(Apps)~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal,data=na.omit(College.train))
summary(Col.lm2)
plot(Col.lm2)

#YL
```
\
Answer: 1) we choose transformation by the the result of powertransform function, as well as the results of question 1.\
2) plots analysis: plot 1: much better: no more obvious pattern. plot 2: the heavyness of the tail is reduced.\
Together, this is a better model. (Notice that the $R^2$ of model 2 is higher than that of model 1)
\

#NEED ANALYSIS
5.  Repeat the predictive checks described in problem 3, but using your model from problem 4.  If you transform the response, you will need to back transform  data to the original units in order to compute the RMSE in the original units.  Does this suggest that the model is adequate?  Do the two graphs provide information about which model is better?
```{r}
nsim=1000
rm2<-c()
n=nrow(College.train)
X2=model.matrix(Col.lm2)
sim.fit2=sim(Col.lm2,nsim)

for (i in 1:nsim)
{

  mu=X2 %*% sim.fit2@coef[i,]
  y.rep=rnorm(n,mean=mu,sd=sim.fit2@sigma[i])
 yt=exp(y.rep)
  newlm<-lm(y.rep~X)
  yhat<-predict.lm(newlm)
 yhat<-exp(yhat)
  rm2[i]<-rmse(yt,yhat)
}
Col.lm.predict2<-exp(predict(Col.lm2,data=College.train))
rmse.observations<-rmse(College.train$Apps,Col.lm.predict2)
pval.lm2=sum(rm2>rmse.observations)/length(rm2)
pval.lm2
df.fit2<-data.frame(rmse.fit2=rm2)
ggplot(df.fit2,aes(x=rmse.fit2))+geom_histogram()+geom_vline(xintercept=rmse.observations,col=2)


```
6. Use your two fitted models to predict the number of applications for the testing data, `College.test`.  Plot the predicted residuals $y_i - \hat{y}_i$  versus the predictions.  Are there any cases where the model does a poor job of predicting?  Compute the RMSE using the test data
where now RMSE = $\sqrt{\sum_{i = 1}^{n.test}(y_i - \hat{y}_i)^2/n.test}$ where the sum is over the test data.  Which model is better for the out of sample prediction?
```{r}
#College.test$Elite<-mapvalues(College.test$Elite,from=c("Yes","No"),to=c(2,1))
#College.test$Private<-mapvalues(College.test$Private,from=c("Yes","No"),to=c(2,1))
#remove these lines because they generate errors.

yhat1<-predict(Col.lm,type="response",newdata=College.test)
yhat2<-exp(predict(Col.lm2,type="response",newdata=College.test))
rmse=function(y,ypred){
  rmse=sqrt(mean((y-ypred)^2))
return(rmse)
}

r1=rmse(College.test$Apps,yhat1)
r2=rmse(College.test$Apps,yhat2)
r1
r2
residual=c()
residual=College.test$Apps-yhat1
residual2=College.test$Apps-yhat2
plot(yhat1,residual)
plot(yhat2,residual2)

```
\
Answer: Model two is better\
For model one, one bad prediction when predicted value is around 15000.\
For model two, there are bad predictions when predicted value is above 15000\
Both models, when the predicted value are big, the residual tends to increase. As a result, the models do a poor job of predicting one predicted value are big.\

7.  As the number of applications is a count variable, a Poisson regression model is a natural alternative for modelling this data.   Build a Poisson model using  main effects and possible interactions/transformations.    Comment on the model adequacy based on diagnostic plots and other summaries. Is there evidence that there is lack of fit?
```{r}
col.glm <- glm(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train,family=poisson(link="log"))
summary(col.glm)
plot(col.glm)
#yl
```
\
Answer: The poisson model is not very good: although plot 1 does not demonstrate pattern, the QQplot shows a heavy tail (lack of fit), and plot 4 shows there are many influential points.\
Looking at the residual deviance, 181277 >> 575 df: also an evidence of lack of fit.
\


#NEED ANALYSIS
8.  Generate 1000 replicate data sets using the coefficients from the Poisson model you fit above.  Using RMSE as a statistic, $\sqrt{\sum_i(y^{\text{rep}} - \hat{y}_i^{\text{rep}})^2/n }$, how does the RMSE from the model based on the training data compare to RMSE's based on the replicated data.  What does this suggest about model adequacy?   Provide a histogram of the RMSE's with a line showing the location of the observed RMSE and compute a p-value.  

\Answer: the RMSE based on the training data is 1172.726. RMSE based on replicated data is 
```{r}
nsim=1000
n=nrow(College.train)
X=model.matrix(col.glm)
class(col.glm)<-"glm"
sim.apps.glm2=sim(col.glm,nsim)
rm3=c()
for(i in 1:nsim){1
  mu=exp(X%*%sim.apps.glm2@coef[i,])
  y.rep=rpois(n,lambda=mu)
   #newlm3<-glm(y.rep~X,family=poisson(link="log"))
   #yhat<-predict(newlm3,type="response")
  
    rm3[i]<-rmse(y.rep,mu)
  }
  

poi.yhat.train=predict(col.glm,type="response")
rmse.observations<-rmse(College.train$Apps,poi.yhat.train)
pval.lm3=sum(rm3>rmse.observations)/length(rm3)
df.fit3<-data.frame(rmse.fit3=rm3)
#ggplot(df.fit3,aes(x=df.fit3$rmse.fit3))+geom_histogram()+geom_vline(xintercept=rmse.observations,col=2)#
#use hist instead. - YL
hist(rm3)
abline(v=rmse.observations,col="red")
pval.lm3


mean(rm3>rmse.observations)

```
9.  Using the test data set, calculate the RMSE for the test data using the predictions from the Poisson model.  How does this compare to the RMSE based on the observed data?  Is this model better than the linear regression models in terms of out of sample prediction?
```{r}
rmse = function(y, ypred) {
  rmse = sqrt(mean((y - ypred)^2))
  return(rmse)
}
col.train.glm <- glm(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train,family=poisson(link="log"))

poi.yhat.train=predict(col.train.glm,type="response")
poi.yhat.test=predict(col.train.glm,newdata=College.test,type="response")

rmse(College.train$Apps, poi.yhat.train)
rmse(College.test$Apps, poi.yhat.test)
#L
```
\
Answer: The RMSE base on traing data is 1173 while the RMSE base on test data is 2568, which indicate the possiblility of over fitting of the model. Also, as the RMSE for linear model of out of sample prediction is 1381.472, the possion model is not better than the linear model.\

10. Build a model using the negative binomial model (consider transformations and interactions if needed) and examine diagnostic plots.  Are there any suggestions of problems with this model?

Answer:The residual deviance is 597.37 and the degree of freedom is 575. Since the residual deviance is very close to the degree of freedom, we can say the model is not lack of fit. Looking at the residual plots, the point scattered equally above and under the line. And there is no points out of the cook's distance indicate there is no influential points. I did not see if there are any problems with this model

```{R}
col.train.nb=glm.nb(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train)
summary(col.train.nb)
plot(col.train.nb)

#L
```

11. Carry out the predictive checks for the negative model model using simulated replicates with RMSE and add RMSE from the test data and observed data to your plot.  What do these suggest about 1) model adequacy and 2) model comparison?  Which model out of all that you have fit do you recommend?  


\
Answer: The mean value of RMSE of simulated replicates under negative binomial model is 1765.1 The RMSE of train data is 1261.002 and the RMSE of test data is 2666.5. Since the RMSE of train data is significantly smaller than RMSE of test data, it might suggest overfitting of the model.
For Possion model, RMSE of train data is 1172.726 and the RMSE of test data is 2568.249. The negative binomial is better than the possion model. I would recommend negative binomial model compare to the possion model.

\
```{r}
rmse = function(y, ypred) {
  rmse = sqrt(mean((y - ypred)^2))
  return(rmse)
}

#col.train.nb=glm.nb(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train)

#nb.yhat.train=predict(col.train.nb,type="response")
#nb.yhat.test=predict(col.train.nb,newdata=College.test,type="response")

#rn<-rmse(College.train$Apps, nb.yhat.train)
#rnt<-rmse(College.test$Apps, nb.yhat.test)
```

```{r}
n = nrow(College.train)
X = model.matrix(col.train.nb)
class(col.train.nb) <- "glm" # over-ride class of "glm.nb"
sim.hiv.nb = sim(col.train.nb, nsim) # use GLM to generate beta's
sim.hiv.nb@sigma = rnorm(nsim, col.train.nb$theta, col.train.nb$SE.theta) # add slot for theta overide sigma
rm4<-c()
for (i in 1:nsim) {
mu = exp(X %*% sim.hiv.nb@coef[i,])
y.rep = rnegbin(n, mu=mu, theta=sim.hiv.nb@sigma[i])
newlm4<-glm.nb(y.rep~X)#???????
  yhat<-predict(newlm4,type="response")
  rm4[i]<-rmse(y.rep,yhat)
}
mean(rm4)

Col.pred4<-predict(col.train.nb,data=College.train,type="response")
rmse.observations.11<-rmse(College.train$Apps,Col.pred4)
pval.lm4=sum(rm4>rmse.observations.11)/length(rm4)
pval.lm4

df.fit4<-data.frame(rmse.fit4=rm4)
ggplot(df.fit4,aes(x=rmse.fit4))+geom_histogram()+geom_vline(xintercept=rmse.observations,col=2)

hist(rm4)
abline(v=rmse.observations.11)
```

12.  While RMSE is a popular summary for model goodness of fit, coverage of confidence intervals is an alternative. For each case in the test set, find a 95% prediction interval.  Now evaluate if the response is in the test data are inside or outside of the intervals.   If we have the correct coverage, we would expect that at least 95\% of the intervals would contain the test cases. Write a function to calculate coverage (the input should be the fitted model object and the test data-frame) and then evaluate coverage for each of the  models that you fit  (the two normal, the  Poisson and the negative binomial).  Include plots of the confidence intervals versus case number ordered by the prediction, with the left out data added as points.  Comment on the plots, highlighting any unusual colleges where the model predicts poorly.
Linear Model 1
```{r}
pi.lm = function(object, newdata, level = 0.95, nsim = 10000){
  n = nrow(newdata)
  X = model.matrix(object, data = newdata)  
  sim.lm.1 = sim(object, nsim)
  y.rep = matrix(NA,nsim,n)
  for (i in 1:nsim) {
    y.rep[i,] =rnorm(n,X %*% sim.lm.1@coef[i,],sim.lm.1@sigma[i])
  }
  pi = t(apply(y.rep, 2, function(x) {quantile(x, c((1-level)/2, .5+level/2))}))
  return (pi)
}

coverage=function(y,pi){
  mean(y >= pi [,1]&y<=pi[,2])
}


pi = pi.lm(Col.lm, College.test)
lm.coverage1 = coverage(College.test$Apps, pi)



df = data.frame(Apps = College.test$Apps, 
                pred = predict(Col.lm,   # training model
                               College.test,       # test data
                               type="response"),  # type of prediction =  exp(X beta)
                lwr = pi[,1], upr=pi[,2]) 
df = df %>% arrange(pred)   # sort by prediction


gp = ggplot(df, aes(x=pred, y=Apps)) + 
     geom_ribbon(aes(ymin = lwr, ymax = upr), 
                fill = "blue", alpha = 0.2) + 
     geom_point(aes(y=Apps)) +
 xlab("Predicted Unprotected Acts after Intervention at end of Study") +
 ylab("Unprotected Acts after Intervention at end of Study") +
 ggtitle("95% Prediction Intervals under Linear Model 2")
print(gp)


```

Linear Model 2
```{r}
pi.lm = function(object, newdata, level = 0.95, nsim = 10000){
  n = nrow(newdata)
  X = model.matrix(object, data = newdata)  
  sim.lm.1 = sim(object, nsim)
  y.rep = matrix(NA,nsim,n)
  for (i in 1:nsim) {
    y.rep[i,] =exp(rnorm(n,X %*% sim.lm.1@coef[i,],sim.lm.1@sigma[i]))
  }
  pi = t(apply(y.rep, 2, function(x) {quantile(x, c((1-level)/2, .5+level/2))}))
  return (pi)
}

coverage=function(y,pi){
  mean(y >= pi [,1]&y<=pi[,2])
}


pi = pi.lm(Col.lm2, College.test)
lm.coverage2 = coverage(College.test$Apps, pi)



df = data.frame(Apps = College.test$Apps, 
                pred = predict(Col.lm2,   # training model
                               College.test,       # test data
                               type="response"),  # type of prediction =  exp(X beta)
                lwr = pi[,1], upr=pi[,2]) 
df = df %>% arrange(pred)   # sort by prediction


gp = ggplot(df, aes(x=pred, y=Apps)) + 
     geom_ribbon(aes(ymin = lwr, ymax = upr), 
                fill = "blue", alpha = 0.2) + 
     geom_point(aes(y=Apps)) +
 xlab("Predicted Unprotected Acts after Intervention at end of Study") +
 ylab("Unprotected Acts after Intervention at end of Study") +
 ggtitle("95% Prediction Intervals under Linear Model 2")
print(gp)


```


```{r}
pi.poi = function(object, newdata, level=.95, nsim=10000) {
  require(mvtnorm)
  n = nrow(newdata)
  X = model.matrix(object, data=newdata)
  beta = rmvnorm(nsim, coef(object), vcov(object))  # use GLM to generate beta's
  y.rep = matrix(NA, nsim, n)

  for (i in 1:nsim) {
    mu = exp(X %*% beta[i,])
    y.rep[i,] = rpois(n, mu)
  }

  pi = t(apply(y.rep, 2, function(x) {
                       quantile(x, c((1 - level)/2, 
                                     .5 + level/2))}))
  return(pi)
}
K=10
f  = ceiling(n/K)  # number of samples in each fold
folds = sample(rep(1:K, f), n)
NB.coverage = rep(NA, K)
poi.coverage = rep(NA, K)
for (i in 1:K) {
 col.train.12 = College[folds != i,]
 col.test.12  = College[folds == i,]
 col.train.poi=glm(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train,family=poisson)

 pi = pi.poi(col.train.poi, col.test.12)
 poi.coverage[i] = coverage(col.test.12$Apps, pi)
}
mean(poi.coverage)
```
```{r}


df = data.frame(Apps = col.test.12$Apps, 
                pred = predict(col.train.poi,   # training model
                               col.test.12,       # test data
                               type="response"),  # type of prediction =  exp(X beta)
                lwr = pi[,1], upr=pi[,2]) 
df = df %>% arrange(pred)   # sort by prediction


gp = ggplot(df, aes(x=pred, y=Apps)) + 
     geom_ribbon(aes(ymin = lwr, ymax = upr), 
                fill = "blue", alpha = 0.2) + 
     geom_point(aes(y=Apps)) +
 xlab("Predicted Unprotected Acts after Intervention at end of Study") +
 ylab("Unprotected Acts after Intervention at end of Study") +
 ggtitle("95% Prediction Intervals under Possion Model")


print(gp)
```


Negative Binomial Model
```{r}

coverage=function(y,pi){
  mean(y >= pi [,1]&y<=pi[,2])
}
pi.nb = function(object, newdata, level=.95, nsim=10000) {
  require(mvtnorm)
  n = nrow(newdata)
  X = model.matrix(object, data=newdata)
  beta = rmvnorm(nsim, coef(object), vcov(object))  # use GLM to generate beta's
  theta = rnorm(nsim, object$theta, object$SE.theta) 
  y.rep = matrix(NA, nsim, n)

  for (i in 1:nsim) {
    mu = exp(X %*% beta[i,])
    y.rep[i,] = rnegbin(n, mu=mu, theta=theta[i])
  }

  pi = t(apply(y.rep, 2, function(x) {
                       quantile(x, c((1 - level)/2, 
                                     .5 + level/2))}))
  return(pi)
}

K = 10
f  = ceiling(n/K)  # number of samples in each fold
folds = sample(rep(1:K, f), n)
NB.coverage = rep(NA, K)
for (i in 1:K) {
 col.train.12 = College[folds != i,]
 col.test.12  = College[folds == i,]
 col.train.nb=glm.nb(Apps ~log(F.Undergrad)+I(perc.alumni^2)+Grad.Rate+log(Expend)+log(Room.Board)+Room.Board:Personal, data=College.train)

 pi = pi.nb(col.train.nb, col.test.12)
 NB.coverage[i] = coverage(col.test.12$Apps, pi)
}
mean(NB.coverage)
```

```{r}
df = data.frame(Apps = col.test.12$Apps, 
                pred = predict(col.train.nb,   # training model
                               col.test.12,       # test data
                               type="response"),  # type of prediction =  exp(X beta)
                lwr = pi[,1], upr=pi[,2]) 
df = df %>% arrange(pred)   # sort by prediction


gp = ggplot(df, aes(x=pred, y=Apps)) + 
     geom_ribbon(aes(ymin = lwr, ymax = upr), 
                fill = "blue", alpha = 0.2) + 
     geom_point(aes(y=Apps)) +
 xlab("Predicted Unprotected Acts after Intervention at end of Study") +
 ylab("Unprotected Acts after Intervention at end of Study") +
 ggtitle("95% Prediction Intervals under Negative Binomial Model")


print(gp)
```
Comments: Looking at all the models. The first model has a coverage of 95.9%. The plot of confidence interval is narrow. For the second linear model, the coverage is 93.3%. The Possion model has a narrow 95% confidence interval but the coverage is very low, indicates that the model is not appropriate.The coverage is 95.5% for Negative binomial model, indicate that the 95% data are in the 95% confidence interval and thus, the negative binomial model is appropriate. The interval of the second linear model is narrower than the nagative model, indicate a more accurate prediction.



13.  Provide a table  with 
the 1) RMSE's on the observed data, 2) RMSE's on the test data, 3) coverage, 4) the predictive check p-value with one row for each of the  models and comment the results.  Which model do you think is best and why?  Consider the job of an administrator who wants to ensure that there are enough staff to handle reviewing applications.  Explain why coverage might be useful.
```{r}
#1 linear model
#1 observed data

#College.train.rmca$Apps
coverage=function(y,pi){
  mean(y >= pi[,1]&y<=pi[,2])
}

results<-matrix(NA,nrow=4,ncol=4)
Col1_fit<-predict(Col.lm)
Col2_fit<-predict(Col.lm2)
Col.glm_fit<-predict(col.glm,type="response")
Col.nb_fit<-predict(col.train.nb,type="response")

RMSE_OB1<-rmse(College.train.rmca$Apps,Col1_fit)
RMSE_OB2<-rmse(College.train$Apps,Col2_fit)
RMSE_GLM<-rmse(College.train$Apps,Col.glm_fit)
RMSE_NB<-rmse(College.train$Apps,Col.nb_fit)

Col1_te<-predict(Col.lm,newdata = College.test)
#Col2_te<-predict(Col.lm2,newdata = College.test)
Col.glm_te<-predict(col.glm,newdata = College.test,type="response")
Col.nb_te<-predict(col.train.nb, newdata=College.test,type="response")

RMSE_TE1<-rmse(College.test$Apps,Col1_te)
RMSE_TE2<-rmse(College.test$Apps,Col2_te)
RMSE_GLM_TE<-rmse(College.test$Apps,Col.glm_te)
RMSE_NB_TE<-rmse(College.test$Apps,Col.nb_te)





results[,1]<-c(RMSE_OB1,RMSE_OB2,RMSE_GLM,RMSE_NB)
results[,2]<-c(RMSE_TE1,RMSE_TE2,RMSE_GLM_TE,RMSE_NB_TE)
results[,4]<-c(pval.lm1,pval.lm2,pval.lm3,pval.lm4)
results[,3]<-c(lm.coverage1,lm.coverage2,mean(poi.coverage),mean(NB.coverage))
rownames(results)<-c("lm1","lm2","poisson","neg_bin")
colnames(results)<-c("RMSE obs","RMSE test","Coverage","p-value")

results
```
\
Answer: I think the best model is the negative binomial, which has a low in-sample RMSE, and the lowest out-of-sample RMSE. The coverage is 95.3%, which is pretty good. Coverage of 95.3% means that 95.5% of the whole sample are within the predicted range, which is a good indicator the total # of applications.
\

#NEED ANALYSIS
14.  For your "best" model  provide a nicely formatted table (use `kable()` or `xtable()`) of relative risks and 95% confidence intervals.  Pick 5 of the most important variables and provide a paragraph that provides an interpretation of the parameters (and intervals) that can be provided to a university admissions officer about which variables increase admissions.  
```{r}
table<-matrix(NA,20,3)
j=1 
for(i in rownames(coefficients(summary(glm3)))){
  table[j,1]<-exp((summary(glm3)$coefficients[j,1])+(summary(glm3)$coefficients[j,2])*1.96)
  table[j,2]<-exp((summary(glm3)$coefficients[j,1])-(summary(glm3)$coefficients[j,2])*1.96)
  table[j,3]<-exp(summary(glm3)$coefficients[j,1])
  j=j+1
}
colnames(table)<-c("lower bound 95% confidence interval","upper bound 95% confidence interval", "relative risk")
rownames(table)<-rownames(coefficients(summary(glm3)))

kable(table)
```
### Some Theory   


15. Gamma mixtures of Poissons:  From class we said that
\begin{align}
Y \mid \lambda & \sim P(\lambda) \\
p(y \mid \lambda) & = \frac{\lambda^y e^{-\lambda}} {y!} \\
& \\
\lambda \mid \mu, \theta & \sim G(\theta, \theta/\mu)  \\
p(\lambda \mid  \mu, \theta) & = \frac{(\theta/ \mu)^\theta}{\Gamma(\theta)} \lambda^{\theta - 1} e^{- \lambda \theta/\mu} \\
& \\
p(Y \mid \mu, \theta) & = \int p(Y \mid \lambda) p(\lambda \mid \theta, \theta/\mu) d \lambda \\
 & =   \frac{ \Gamma(y + \theta)}{y! \Gamma(\theta)}
\left(\frac{\theta}{\theta + \mu}\right)^{\theta}
\left(\frac{\mu}{\theta + \mu}\right)^{y} \\
Y \mid \mu, \theta & \sim NB(\mu, \theta) 
\end{align}
Derive the density of $Y \mid \mu, \theta$ in (8) showing your work using LaTeX expressions.  (Note this may not display if the output format is html, so please use pdf.)
Using iterated expectations with the Gamma-Poisson mixture, find the mean and variance of $Y$, showing your work.

$$p(Y\mid \mu,\theta) =\int p(Y\mid \lambda) * p(\lambda | \theta, \theta/\mu)* d\lambda$$
$$= \int \frac{\lambda^y * e^{-\lambda}}{y!}*\frac{(\theta/\mu)^{\theta}}{\gamma(\theta)}*\lambda^{\theta-1}*e^{-\lambda*\theta/\mu}d\lambda$$ 
$$=\frac{(\theta/\mu)^\theta}{y!\gamma(\theta)}*\int\lambda^{y+\theta-1}*e^{-\lambda(1+\theta/\mu)d\lambda}$$
$$=\frac{(\theta/\mu)^\theta}{y!\gamma(\theta)}*\frac{\gamma(y+\theta)}{(1+\theta/\mu)^{y+\theta}}$$
$$=\frac{\gamma(y+\theta)}{y!*\gamma(\theta)}*(\frac{\theta}{\theta+\mu})^\theta*(\frac{\mu}{\mu+\theta})^y$$
$$\propto Negative Binomial(\mu,\theta)$$


$$E(Y\mid \mu,\theta)=E[E(Y|\lambda,\mu,\theta)|\mu,\theta]$$
$$=E[\lambda|\mu,\theta]$$
$$=\mu$$ Because $\lambda$ is a gamma distribution.


$$Var(Y|\mu,\theta)=E[Var(Y|\mu,\theta)\mid\mu,\theta]+Var[E(Y|\mu,\theta)\mid \mu,\theta]$$
$$=E[\lambda\mid\mu,\theta]+Var[\lambda\mid\mu,\theta]$$
$$=\mu-\mu^2/\theta$$
#YL



